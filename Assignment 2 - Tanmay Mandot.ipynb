{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3614fd",
   "metadata": {},
   "source": [
    "## Question 1 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Initialize variables\n",
    "base_url = \"https://press.un.org/en\"\n",
    "seed_url = \"https://press.un.org\"\n",
    "press_release_links = []\n",
    "crisis_press_releases = []\n",
    "\n",
    "# Function to extract links from a page - important \n",
    "def extract_links(url):\n",
    "    response = urlopen(url)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    return [link[\"href\"] for link in links]\n",
    "\n",
    "# Function to check if a page contains the word \"crisis\"\n",
    "def contains_crisis(url):\n",
    "    response = urlopen(url)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return \"crisis\" in text.lower()\n",
    "\n",
    "# Start with the base URL\n",
    "press_release_links.extend(extract_links(base_url))\n",
    "\n",
    "# Iterate through the links\n",
    "for link in press_release_links:\n",
    "    if link.startswith(\"/en/\") and link.endswith(\".doc.htm\"):\n",
    "        full_url = seed_url + link\n",
    "        try:\n",
    "            if contains_crisis(full_url):\n",
    "                crisis_press_releases.append(full_url)\n",
    "                # Stop when you have exactly 10 press releases\n",
    "                if len(crisis_press_releases) == 10:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        # Introduce a delay between requests to avoid overloading the server\n",
    "        time.sleep(1)  # You can adjust the delay as needed\n",
    "\n",
    "# Save the HTML source code of each press release\n",
    "for index, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    response = urlopen(press_release)\n",
    "    html_content = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # Create the file name using the specified format\n",
    "    file_name = f\"{'1'}_{index}.txt\"\n",
    "\n",
    "    # Save the HTML source code to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "        print(f\"Saved {file_name}\")\n",
    "\n",
    "# Print the list of press releases containing \"crisis\"\n",
    "for index, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    print(f\"Press Release {index}: {press_release}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c9361",
   "metadata": {},
   "source": [
    "## Question 1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.europarl.europa.eu/news/en/press-room/page/\"\n",
    "page_number = 0  # Initialize the page number\n",
    "press_release_links = []\n",
    "crisis_press_releases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0493eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize variables\n",
    "base_url = \"https://www.europarl.europa.eu/news/en/press-room/page/0\"\n",
    "press_release_links = []\n",
    "crisis_press_releases = []\n",
    "\n",
    "# Function to extract links from a page\n",
    "def extract_links(url):\n",
    "    try:\n",
    "        request = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response = urlopen(request)\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        return [link[\"href\"] for link in links]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_links: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to check if a page contains the word \"crisis\"\n",
    "def contains_crisis(url):\n",
    "    try:\n",
    "        request = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response = urlopen(request)\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        return \"crisis\" in text.lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in contains_crisis: {e}\")\n",
    "        return False\n",
    "\n",
    "# Extract links from the initial page\n",
    "press_release_links.extend(extract_links(base_url))\n",
    "\n",
    "# Process links\n",
    "for link in press_release_links:\n",
    "    if contains_crisis(link):\n",
    "        crisis_press_releases.append(link)\n",
    "\n",
    "# Print the list of press releases containing \"crisis\"\n",
    "for idx, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    print(f\"Press Release {idx}: {press_release}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33148e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize variables\n",
    "base_url = \"https://www.europarl.europa.eu/news/en/press-room/page/{}\"\n",
    "page_number = 0\n",
    "press_release_links = []\n",
    "crisis_press_releases = []\n",
    "\n",
    "# Function to extract links from a page\n",
    "def extract_links(url):\n",
    "    try:\n",
    "        request = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response = urlopen(request)\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        return [link[\"href\"] for link in links]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_links: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to check if a page contains the word \"crisis\"\n",
    "def contains_crisis(url):\n",
    "    try:\n",
    "        request = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response = urlopen(request)\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        return \"crisis\" in text.lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in contains_crisis: {e}\")\n",
    "        return False\n",
    "\n",
    "# Continuously send requests and process links until no more press releases are found\n",
    "while True:\n",
    "    current_url = base_url.format(page_number)\n",
    "    \n",
    "    # Extract links from the current page\n",
    "    press_release_links.extend(extract_links(current_url))\n",
    "    \n",
    "    # Process links\n",
    "    page_processed = False  # Flag to check if the current page has been processed\n",
    "    for link in press_release_links:\n",
    "        if contains_crisis(link):\n",
    "            crisis_press_releases.append(link)\n",
    "            page_processed = True\n",
    "\n",
    "    # Check if the current page has no more press release links\n",
    "    if not page_processed:\n",
    "        break\n",
    "    \n",
    "    # Increment the page number for the next request\n",
    "    page_number += 1\n",
    "\n",
    "# Save the content of press releases in text files\n",
    "for idx, press_release_url in enumerate(crisis_press_releases, start=1):\n",
    "    try:\n",
    "        request = Request(press_release_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response = urlopen(request)\n",
    "        html_content = response.read().decode(\"utf-8\")\n",
    "\n",
    "        # Create the file name using the specified format\n",
    "        file_name = f\"press_release_{idx}.txt\"\n",
    "\n",
    "        # Save the HTML source code to the file\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_content)\n",
    "            print(f\"Saved {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving press release {idx}: {e}\")\n",
    "\n",
    "# Print the list of press releases containing \"crisis\"\n",
    "for idx, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    print(f\"Press Release {idx}: {press_release}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7773027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
