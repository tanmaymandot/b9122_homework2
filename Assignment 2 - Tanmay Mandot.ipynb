{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9ca059",
   "metadata": {},
   "source": [
    "## Question 1 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ff42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Starting variables\n",
    "base_url = \"https://press.un.org/en\"\n",
    "seed_url = \"https://press.un.org\"\n",
    "press_release_links = []\n",
    "crisis_press_releases = []\n",
    "\n",
    "# Function to extract links from a page\n",
    "def extract_links(url):\n",
    "    response = urlopen(url)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    return [link[\"href\"] for link in links]\n",
    "\n",
    "# Function to check if a page contains the word \"crisis\"\n",
    "def contains_crisis(url):\n",
    "    response = urlopen(url)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return \"crisis\" in text.lower()\n",
    "\n",
    "# Start with the base URL\n",
    "press_release_links.extend(extract_links(base_url))\n",
    "\n",
    "# Iterate through the links\n",
    "for link in press_release_links:\n",
    "    if link.startswith(\"/en/\") and link.endswith(\".doc.htm\"):\n",
    "        full_url = seed_url + link\n",
    "        try:\n",
    "            if contains_crisis(full_url):\n",
    "                crisis_press_releases.append(full_url)\n",
    "                # Stop when you have exactly 10 press releases\n",
    "                if len(crisis_press_releases) == 10:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        # Introduce a delay between requests to avoid overloading the server\n",
    "        time.sleep(1)  # You can adjust the delay as needed\n",
    "\n",
    "# Save the HTML source code of each press release\n",
    "for index, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    response = urlopen(press_release)\n",
    "    html_content = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # Create the file name using the specified format\n",
    "    file_name = f\"{'1'}_{index}.txt\"\n",
    "\n",
    "    # Save the HTML source code to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "        print(f\"Saved {file_name}\")\n",
    "\n",
    "# Print the list of press releases containing \"crisis\"\n",
    "for index, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    print(f\"Press Release {index}: {press_release}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d980540",
   "metadata": {},
   "source": [
    "## Question 1 b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ee399",
   "metadata": {},
   "source": [
    "##### This takes like 1.5 minutes to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600011b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2_1.txt\n",
      "Saved 2_2.txt\n",
      "Saved 2_3.txt\n",
      "Saved 2_4.txt\n",
      "Saved 2_5.txt\n",
      "Saved 2_6.txt\n",
      "Saved 2_7.txt\n",
      "Saved 2_8.txt\n",
      "Saved 2_9.txt\n",
      "Saved 2_10.txt\n",
      "Press Release 1: https://www.europarl.europa.eu/news/en/press-room/20231006IPR06504/where-there-is-a-political-will-there-is-a-way-migration-enlargement-budget\n",
      "Press Release 2: https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
      "Press Release 3: https://www.europarl.europa.eu/news/en/press-room/20230929IPR06130/parliament-argues-for-a-top-up-to-multi-annual-budget-for-crisis-response\n",
      "Press Release 4: https://www.europarl.europa.eu/news/en/press-room/20230904IPR04608/spanish-presidency-debriefs-ep-committees-on-priorities\n",
      "Press Release 5: https://www.europarl.europa.eu/news/en/press-room/20230918IPR05429/meps-argue-for-a-top-up-to-multi-annual-budget-for-crisis-response\n",
      "Press Release 6: https://www.europarl.europa.eu/news/en/press-room/20230911IPR04923/reduce-demand-and-protect-people-in-prostitution-say-meps\n",
      "Press Release 7: https://www.europarl.europa.eu/news/en/press-room/20230911IPR04918/svietlana-tsikhanouskaya-to-meps-support-belarusians-european-aspirations\n",
      "Press Release 8: https://www.europarl.europa.eu/news/en/press-room/20230911IPR04908/meps-vote-to-strengthen-eu-defence-industry-through-common-procurement\n",
      "Press Release 9: https://www.europarl.europa.eu/news/en/press-room/20230904IPR04619/meps-back-plans-to-protect-consumers-from-energy-market-manipulation\n",
      "Press Release 10: https://www.europarl.europa.eu/news/en/press-room/20230717IPR03028/meps-back-plans-for-a-more-affordable-and-consumer-friendly-electricity-market\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL template with a placeholder for the page number\n",
    "base_url_template = \"https://www.europarl.europa.eu/news/en/press-room/page/{}\"\n",
    "press_release_links = []\n",
    "crisis_press_releases = []\n",
    "\n",
    "# Function to extract links from a page\n",
    "def extract_links(url):\n",
    "    try:\n",
    "        request = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response = urlopen(request)\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        return [link[\"href\"] for link in links]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_links: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to check if a page contains the word \"crisis\"\n",
    "def contains_crisis(url):\n",
    "    try:\n",
    "        request = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response = urlopen(request)\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        return \"crisis\" in text.lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in contains_crisis: {e}\")\n",
    "        return False\n",
    "\n",
    "# Iterate through pages by incrementing page_number\n",
    "page_number = 0\n",
    "while True:\n",
    "    current_url = base_url_template.format(page_number)\n",
    "    page_links = extract_links(current_url)\n",
    "    \n",
    "    if not page_links:\n",
    "        break  # No more pages to scrape\n",
    "    \n",
    "    for link in page_links:\n",
    "        if contains_crisis(link):\n",
    "            crisis_press_releases.append(link)\n",
    "    \n",
    "    # Increment the page number for the next page\n",
    "    page_number += 1\n",
    "    if page_number >= 5: \n",
    "         break\n",
    "            \n",
    "# Save the HTML source code of each press release\n",
    "for index, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    response = urlopen(press_release)\n",
    "    html_content = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # Create the file name using the specified format\n",
    "    file_name = f\"{'2'}_{index}.txt\"\n",
    "\n",
    "    # Save the HTML source code to the file\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "        print(f\"Saved {file_name}\")\n",
    "\n",
    "# Print the list of press releases containing \"crisis\"\n",
    "for idx, press_release in enumerate(crisis_press_releases, start=1):\n",
    "    print(f\"Press Release {idx}: {press_release}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
